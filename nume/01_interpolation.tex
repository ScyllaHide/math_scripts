\section{Interpolation}
\subsection{Aufgabenstellung}
\textbf{Gegeben:} $n+1$ Datenpaare $(x_0, f_0), \ldots, (x_n,f_n)$ reeller Zahlen. Die Zahlen $x_0, \ldots, x_n$ heißen \emph{Stützstellen} und die Zahlen $f_0, \ldots, f_n$ \emph{Stützwerte}.

Die Grundaufgabe der Interpolation besteht darin, eine Funktion $F: \real \to \real$ zu finden, die den folgenden \emph{Interpolationsbedingungen}
\begin{equation}
 F(x_0) = f_0,\, \ldots,\, F(x_n) = f_n \label{eq:int_bedingung}
\end{equation}
genügt. In Erweiterung dessen können etwa auch Bedingungen an Ableitungen von $F$ an bestimmten Stellen vorkommen.

Eine solche Funktion wird \emph{Interpolierende} genannt. Es ist sehr leicht, diese Aufgabe zu lösen. Zum Beispiel ist
\[ f(x) = \begin{cases}
           0, &\text{falls } x \notin \{ x_0, \ldots, x_n \}, \\
           f_0, &\text{falls } x = x_0, \\
            &\vdots \\
           f_n, &\text{falls } x = x_n.
          \end{cases} \]
immer eine Interpolierende. Zur sinnvollen Bearbeitung der Interpolationsaufgabe muss man also die Menge der als Interpolierende zugelassenen Funktionen einschränken, zum Beispiel auf Polynome, Winkel- oder Exponentialfunktionen oder Spline-Funktionen. Im Zusammenhang mit der Wahl der Funktionenmenge (eigentlich des Funktionenraums) sind folgende Fragen von Bedeutung:
\begin{itemize}
 \item Existenz einer Interpolierenden,
 \item Eindeutigkeit der Interpolierenden,
 \item Weitere Eigenschaften (zum Beispiel hinsichtlich Krümmung, Approximation einer Funktion $f$ (wenn $f = f(x_k)$ für $k=0,\ldots,n$)
 \item Wie sollte man die Stützstellen wählen, wenn sie nicht vorgegeben sind?
 \item Effizienz der Bestimmung einer Interpolierenden.
\end{itemize}

\begin{exmp}
 Die Temperatur bei Erwärmung eines Kessels wurde zu diskreten Zeitpunkten tabelliert:
 
 \begin{center}
 \begin{tabular}{c|cccccc}
  k & 0 & 1 & 2 & 3 & 4 & 5\\
  \hline
  $x_k$ in \si{\s} & 0 & 1 & 2 & 3 & 4 & 5\\
  $f_k$ in \si{\celsius} & 80 & 85.8 & 86.4 & 93.6 & 98.3 & 99.1
 \end{tabular}
 \end{center}

 Beispielsweise soll die Interpolierende im
 \begin{enumerate}[a)]
  \item Raum der stetigen stückweise affinen Funktionen,
  \item Raum der Polynome höchstens 5. Grades,
  \item Raum der Polynome höchstens 4. Grades
 \end{enumerate}
 ermittelt werden.
\end{exmp}

\subsection{Interpolation durch Polynome}
Sei $\Pi_n$ der Vektorraum der Polynome vom Höchstgrad $n$ mit der üblichen Addition $(p_1+p_2)(x) := p_1(x) + p_2(x)$ und der üblichen skalaren Multiplikation $(\alpha p)(x) := \alpha \cdot p(x)$.

Für jedes $p \in \Pi_n$ gibt es dann $a_0, \ldots, a_n \in \real$, so dass 
\begin{equation} p(x) = a_n x^n + a_{n-1} x^{n-1} + \ldots + a_1 x + a_0 \text{ für alle } x \in \real. \end{equation}

\subsubsection{Existenz und Eindeutigkeit}
\begin{thm}
 Zu $n+1$ beliebigen Datenpaaren $(x_0, f_0), \ldots, (x_n, f_n)$ mit paarweise verschiedenen Stützstellen existiert genau ein Polynom $p \in \Pi_n$, das die Interpolationsbedingungen (\ref{eq:int_bedingung}) erfüllt.
\end{thm}

Dieses Polynom heißt \emph{Interpolationspolynom} zu den Datenpaaren $(x_0, f_0), \ldots, (x_n, f_n)$.

\begin{proof}
 \begin{enumerate}[a)]
  \item Existenznachweis durch Konstruktion.
  
  Sei $j \in \{ 0, \ldots, n \}$ und bezeichne $L_j: \real \to \real$ mit
  \[ L_j(x) := \prod_{\substack{i=0,\\i \ne j}}^n \frac{x-x_i}{x_j-x_i} = \frac{(x-x_0)\cdots(x-x_{j-1})(x-x_{j+1})\cdots(x-x_n)}{(x_j-x_0)\cdots(x_j-x_{j-1})(x_j-x_{j+1})\cdots(x_j-x_n)} \]
  das \emph{Lagrange-Basispolynom} vom Grad $n$. Offenbar gilt $L_j \in \Pi_n$ und
  \begin{equation} L_j(x_k) = \delta_{jk} = \begin{cases}
                 1, &\text{falls } k = j, \\
                 0, &\text{falls } k \ne j.
                \end{cases} \end{equation}
  Definiert man $p: \real \to \real$ durch
  \begin{equation} p(x) = \sum_{j=0}^n f_j \cdot L_j(x) \text{ für alle } x \in \real, \end{equation}
  so ist $p \in \Pi_n$. Außerdem erfüllt $p$ wegen (1.3) die Interpolationsbedingung (1.1).
  \item Nachweis der Eindeutigkeit.
  
  Angenommen, es existieren zwei Interpolierende $p$ und $\tilde{p} \in \Pi_n$ und $p \ne \tilde{p}$. Dann folgt 
  \[ p - \tilde{p} \in \Pi_n \text{ und } (p-\tilde{p})(x_k) = p(x_k) - \tilde{p}(x_k) = 0 \text{ für } k = 0, \ldots, n. \]
  Das heißt, $p - \tilde{p}$ ist vom Höchstgrad $n$ und hat mindestens $n+1$ Nullstellen. Also muss $p-\tilde{p}$ das Nullpolynom sein. Widerspruch zur Annahme $p \ne \tilde{p}$!
 \end{enumerate}
\end{proof}

\begin{rmrk}
 \begin{enumerate}[a)]
  \item Die Darstellung (1.4) heißt \emph{Lagrange-Form} des Interpolationspolynoms.
  \item Um mit (1.4) einen ersten Funktionswert $p(x)$ zu berechnen, werden $O(n^2)$ benötigt. Bei äquidistanten Stützstellen genügen $O(n)$ Operationen.
  
  Ändern sich nur die Stützwerte (die Stützstellen und die Stelle $x$ bleiben gleich), dann kann man $p(x)$ in $O(n)$ ermitteln.
  \item Man kann zeigen, dass die Lagrange-Basispolynome $L_0, \ldots, L_n$ eine Basis des Vektorraums $\Pi_n$ bilden.
 \end{enumerate}
\end{rmrk}

\subsubsection{Newton-Form des Interpolationspolynoms}
Ein Polynom $p \in \Pi_n$ von der Form 
\begin{equation}
 p(x) = c_0 + c_1(x-x_0) + c_2(x-x_0)(x-x_1) + \ldots + c_n (x-x_0)(x-x_1) \ldots (x-x_{n-1}) 
\end{equation}
mit geeigneten Koeffizienten $c_0, \ldots, c_n \in \real$, das die Bedingung (1.1) erfüllt, wird als Interpolationspolynom in \emph{Newton-Form} bezeichnet.

Die Berechnung der Koeffizienten $c_0, \ldots, c_n$ kann rekursiv durch Ausnutzung der Interpolationsbedingungen erfolgen:
\[ f_0 \overset{!}{=} p(x_0) = c_0, \text{ also } c_0 = f_0. \] 
Seien $c_0, \ldots, c_{j-1}$ bereits bestimmt. Dann folgt
\begin{align*}
 f_j \overset{!}{=} p(x_j) = c_0\, &+ c_1 (x_j-x_0) + \ldots + c_{j-1} (x_j-x_0) \ldots (x_j-x_{j-2}) \\
   &+ c_j (x_j-x_0)(x_j-x_1) \ldots (x_j-x_{j-1}) \\
   &+ c_{j+1} (x_j-x_0)(x_j-x_1) \ldots \underbrace{(x_j-x_j)}_{=0}(x_j-x_{j+1} ) + \ldots . 
\end{align*}
Alle Terme ab $c_{j+1}$ entfallen, da sie $(x_j-x_j)=0$ enthalten. Damit lässt sich $c_j$ berechnen:
\[ c_j = \frac{f_j - \left( c_0 + \sum_{k=1}^{j-1} c_k (x_j-x_0)\cdots(x_j-x_{k-1}) \right)}{(x_j-x_0) \cdots (x_j-x_{j-1})} \]

\begin{rmrk}
 \begin{enumerate}[(a)]
  \item Zum Aufwand: Berechnung von $c_0, \ldots, c_n$ (also bei $n+1$ Stütz\-stellen): $O(n^2)$ Operationen. Vorteil der Newton-Form: Kommt ein Datenpaar hinzu, also $(x_{n+1}, f_{n+1})$, dann kann man den Ansatz um einen Summanden erweitern $c_{n+1}( x-x_0 ) \cdot \ldots \cdot (x - x_{n-1})(x - x_n)$. Der zusätzliche Koeffizient $c_{n+1}$ kann mit $O(n)$ Operationen bestimmt werden.
  \item Sind $c_0, \ldots, c_n$ bekannt, dann genügen $O(n)$ Operationen (genauer: $3n$) zur Berechnung von $p(x)$ an der Stelle $x$ (vgl. Algorithmus 1.1).
  \item Die Polynome $N_0, \ldots, N_n:R \to \real$ mit $N_0(x) \equiv 1$, $N_j(x) = (x-x_0) \cdot \ldots \cdot (x - x_{j-1})$, $j = 1, \ldots, n$ heißen \emph{Newton-Basispolynome} und bilden eine Basis des Vektorraums $\Pi_n$.
 \end{enumerate}
\end{rmrk}

Effiziente Berechnung von $p(x)$ beruht auf $(n=3)$:
\begin{align*}
 p(x) &= c_0 + c_1 (x-x_0) + c_2 (x-x_0)(x-x_1) + c_3 (x-x_0)(x-x_1)(x-x_2) \\
      &= c_0 + (x-x_0) ( c_1 + (x-x_1) ( c_2 + (x-x_2) c_3 ) ).
\end{align*}

\begin{algn}[Horner-Schema für die Newton-Form des Interpolationspolynoms]
\begin{lstlisting}
Input: n, x, c_0, ..., c_n, x_0, ..., x_n-1

p:= c_n
FOR j = n-1, n-2, ... 0 DO
  p := c_j + (x-x_j) p
  
OUTPUT: p = p(x)
\end{lstlisting}
\end{algn}

\subsubsection{Interpolationsfehler}
Durch $\| g \|_\infty := \max | g(x) |$ für alle $g \in C[a,b]$ ist eine Norm (die \emph{Maximums-Norm}) in $C[a,b]$ definiert.

\begin{thm}
 Sei $f \in C[a,b]$. Dann existiert zu $\eps > 0$ ein Polynom $p$ mit
 \[ \| f-p \|_\infty \le \eps, \]
 das heißt die Polynome (beliebigen Grades) liegen dicht in $C[a,b]$.
\end{thm}

\emph{Vermutung:} $\| p_n - f \|_\infty \to 0$ für $n \to \infty$, falls $p_n$ das Interpolationspolynom von $f$ ist. 

\emph{Aber:} Gegeben: Stützstellensystem $a \le x_0^{(n)} < x_1^{(n)} < \ldots < x_{n-1}^{(n)} < x_n^{(n)} \le b$, $n = 0, 1, 2, \ldots$

Bezeichne $p_k \in \Pi_n$ das zu den Datenpaaren $(x_k^{(n)}, f(x_k^{(n)}))$, $k = 0, \ldots, n$ gehörige eindeutig bestimmte Interpolationspolynom.

\begin{thm}
 Zu jedem Stützstellensystem gibt es ein $f \in C[a,b]$, so dass \emph{nicht} gilt
 \[ \| f-p_n \|_\infty \to 0 \text{ für } n \to \infty. \]
\end{thm}

Nach einem Resultat von Erdös/Vertesi (1980) gilt sogar, dass es ein $f \in C[a,b]$ gilt, so dass $p_n(x)$ fast überall nicht gegen $f$ konvergiert.

\begin{exmp}[Runge]
 Schlechte Funktion $f$ für äquidistante Stützstellenwahl in $[-1,1]$:
 \[ f: \real \to \real \text{ mit } f(x) = \rez{1 + 25 x^2} \]
 \[ x_0^{(n)} = -1, x_n^{(n)} = 1, x_j^{(n)} = -1 + j h_n \text{ mit } h_n = \frac{2}{n}, j = 1, \ldots, n-1 \]
\end{exmp}

\emph{Aber}:

\begin{thm}[Interpolationsfehler]
 Sei $f \in C^{n+1}[a,b]$. und gelte $a \le x_0 < x_1 < \ldots < x_{n-1} < x_n \le b$. Mit $p_n \in \Pi_n$ werde das zu den Datenpaaren $(x_j, f(x_j))$, $j=1, \ldots, n$ gehörige Interpolationspolynom bezeichnet. Dann existiert zu jedem $x \in [a,b]$ eine Zahl $\xi(x) \in (a,b)$, so dass
 \[ f(x) - p_n(x) = \frac{f^{(n+1)}(\xi(x))}{(n+1)!} w(x), \]
 wobei $w : \real \to \real$ gegeben ist durch
 \[ w(x) = (x-x_0) \cdot \ldots \cdot (x-x_n). \]
\end{thm}

\begin{proof}
 Für $x=x_k$, $k =0, \ldots, n$ klar!
 
 Sei nun $x \in [a,b]$ fest mit $x \notin \{ x_0, \ldots, x_n \}$. Weiter sei $F:[a,b] \to \real$ mit
 \[ F(t) := f(t) - p_n(t) + K(x) \cdot w(t) \text{ für alle } t \in [a,b] \]
 für ein noch zu bestimmendes $K = K(x)$, so dass $F(x) = 0$ gilt. Außerdem ist $F(x_j) = 0$ für $j = 0, \ldots, n$ nach Interpolationsbedingung.
 
 Also hat $F$ mindestens $n+2$ Nullstellen in $[a,b]$. Da offensichtlich $F \in C^{n+1}[a,b]$ ist, erhält man nach $n+1$-facher Anwendung des Satzes von Rolle, dass $F^{(n+1)}$ mindestens eine Nullstelle $\xi(x)$ in $(a,b)$ hat.
 \[ 0 = F^{(n+1)}(\xi(x)) \overset{!}{=} f^{(n+1)}(\xi(x)) - \underbrace{p_n^{(n+1)}(\xi(x))}_{=0} - K(x) \underbrace{w^{(n+1)}(\xi(x))}_{=(n+1)!} \]
 \[ \Rightarrow K(x) = \frac{f^{(n+1)}(\xi(x))}{(n+1)!}. \]
\end{proof}

\begin{exmp}
 Sei $f \in C^2[a,b]$, $\| f'' \|_\infty \le M$, $a = x_0 < x_1 = x_0 + h = b$. Dann gilt für $\lambda(x) \in [0,1]$
 \[ | f(x) - p_1(x) | \le \rez{2} M | (x-x_0)(x-x_1) | = \rez{2} M \lambda(x) h (1-\lambda(x)) h \le \rez{8} M h^2, \]
 für alle $x \in [a,b]$.
\end{exmp}

Weitere Fragen bei der Interpolation:
\begin{itemize}
 \item Forderungen an die Ableitungswerte?
 \item Wahl der Stützstellen? Gibt es optimale Stützstellen zur Fehlerminimierung?
\end{itemize}

\subsection{Interpolation durch Polynomsplines}
\subsubsection{Polynomsplines}
Sei $\Delta$ eine Zerlegung des Intervalls $[a,b]$ durch Stützstellen $a := x_0 < x_1 < \ldots < x_n := b$.

\begin{defn}
 Ein \emph{Polynomspline} vom Grad $m \in \nat$ und Glattheit $l \in \nat$ zur Zerlegung $\Delta$ ist eine Funktion $s \in C^k[a,b]$ mit
 \[ s_k := s |_{[x_k,x_{k+1}]} \in \Pi_m, k = 0, \ldots, n-1. \]
 Dabei ist $s |_{[x_k,x_{k+1}]}$ die \emph{Einschränkung} von $s$ auf das Intervall $[x_k,x_{k+1}]$.
 
 $S_m^l(\Delta)$ ist die Menge aller Polynomsplines vom Grad $m$ und Glattheit $l$ (zur Zerlegung $\Delta$). $S_m^l(\Delta)$ ist ein Vektorraum.
\end{defn}

\subsubsection{Interpolation durch kubische Polynomsplines}
\emph{Gegeben:} Zerlegung $\Delta$ und Stützwerte $f_0, \ldots, f_n$ mit $f_j = f(x_j)$.

\emph{Gesucht:} $s \in S_3^l(\Delta)$ mit 
\begin{equation} s(x_k) = f_k \text{ für } k = 0, \ldots, n. \end{equation}

Exisitiert ein solches Spline für $l=1$ bzw. $l=2$? Eindeutigkeit? Der Nachweis erfolgt konstruktiv.

Abkürzung 
\[ h_k := x_{k+1} - x_k \quad \text{für } k = 0, \ldots, n-1 \]
und 
\[ m_k := s'(x_k) \quad \text{für } k = 0, \ldots, n. \]

Wegen $l \in \{ 1,2 \}$ ist $s$ mindestens stetig differenzierbar. Da $s_k := s|_{[x_k, x_{k+1}]}$ ein Polynom höchstens dritten Grades ist, Ansatz:
\begin{equation}
 s_k(x) = a_k (x-x_k)^3 + b_k (x-x_k) + c_k (x-x_k) + d_k.
\end{equation}

Aus den Interpolationsbedingungen (1.6) und der stetigen Differenzierbarkeit aller Funktionen in $s \in S_3^\ell$ für $\ell \ge 1$ ergeben sich diese Forderungen an $s_k$: 
\begin{equation}
 s_k(x_k) = f_k, \quad s_k(x_{k+1}) = f_{k+1}, \quad s'_k (x_k) = m_k, \quad s'_k (x_{k+1}) = m_{k+1}.
\end{equation}
Mit (1.7) ergeben sich $d_k$ und $c_k$:
\begin{equation}
 d_k = s(x_k) = f_k, \quad c_k = s'(x_k) = m_k
\end{equation}
und damit
\[ s_k( x_{k+1} ) = a_k h_k^3 + b_k h_k^2 + m_k h_k + f_k = f_{k+1} \]
sowie
\[ s'_k(x_{k+1}) = 3 a_k h_k^2 + 2 b_k h_k + m_k = m_{k+1}. \]

Damit ergeben sich $a_k, b_k$ als eindeutige Lösung des Gleichungssystems
\begin{equation}
 \begin{pmatrix} h_k^3 & h_k^2 \\ 3 h_k^2 & 2 h_k \end{pmatrix}
 \begin{pmatrix} a_k \\ b_k \end{pmatrix} =
 \begin{pmatrix} f_{k+1} - f_k - m_k h_k \\ m_{k+1} - m_k \end{pmatrix},
\end{equation}
weil die Systemmatrix $\begin{pmatrix} h_k^3 & h_k^2 \\ 3 h_k^2 & 2 h_k \end{pmatrix}$ immer regulär ist.

\begin{thm}
 Sei eine Zerlegung $\Delta$ des Intervalls $[a,b]$ gegeben. Dann gibt es für beliebig gewählte relle Zahlen 
 \[ f_0, \ldots, f_n \text{ und } m_0, \ldots, m_n, \]
 genau ein Interpolationspolynom $s \in S_3^1(\Delta)$, so dass die Interpolationsbedingung (1.6) und 
 \[ s'(x_j) = m_j, \quad j = 0, \ldots, n \]
 erfüllt sind. Außerdem gilt
 \[ s = s|_{[x_k, x_{k+1}]} = s_k \text{ für } k = 0, \ldots, n-1 \]
 mit $s_k$ aus (1.7) und $a_k, b_k, c_k, d_k$ aus (1.9) und (1.10).
\end{thm}

Wahl der $m_k$?
\begin{itemize}
 \item Falls Ableitungswerte bekannt, dann $m_j = f'(x_j)$, $j=0, \ldots, n$.
 \item Wähle $m_0, \ldots, m_n$ so, dass $s$ zweimal stetig differenzierbar ist, also $s \in S_3^2(\Delta)$ anstelle von $S_3^1(\Delta)$.
\end{itemize}

\subsubsection{Interpolation mit kubischen \texorpdfstring{$C^2$}{C2}-Splines}
Damit $s \in S_3^2(\Delta)$ gilt, muss neben (1.8) auch die Stetigkeit von $s''$ an den Stützstellen gewährleistet sein. Also ergeben sich die Bedingungen
\[ s''_k(x_{k+1}) = s''_{k+1}(x_{k+1}) \quad \text{für } k = 0, \ldots, n-2. \]

Mit (1.7) ergibt sich $s''_k(x) = 6 a_k (x-x_k) + 2 b_k$ für $x \in [x_k,x_{k+1}]$ und damit
\[ s''_k(x_{k+1}) = 6 a_k h_k + 2 b_k \text{ und }
   s''_{k+1}(x_{k+1}) = 2 b_{k+1}, \]
also
\begin{equation}
 3 a_k h_k + b_k \overset{!}{=} b_{k+1}, \quad \text{für } k=0, \ldots, n-2.
\end{equation}

Aus (1.10) folgt
\[ a_k = - \frac{2}{h_k^3} (f_{k+1} - f_k) + \rez{h_k^2} (m_k + m_{k+1} ) \tag{$\circ$} \]
und
\[ b_k = \frac{3}{h_k^2} (f_{k+1} - f_k) - \rez{h_k} (2 m_k + m_{k+1} ) \tag{$\circ\circ$}\]
für $k = 0, \ldots, n-1$. Wegen (1.11) folgt für $k = 0, \ldots, n-2$
\begin{align*}
 &- \frac{6}{h_k} (f_{k+1} - f_k) + \frac{3}{h_k} (m_k + m_{k+1} ) + \frac{3}{h_k^2} (f_{k+1} - f_k) - \rez{h_k} (2 m_k + m_{k+1} ) \\
 &= \frac{3}{h_{k+1}^2} (f_{k+2} - f_{k+1}) - \rez{h_{k+1}} (2 m_{k+1} + m_{k+2} ).
\end{align*}

Damit folgt
\[ \rez{h_k} (m_k + 2 m_{k+1}) + \rez{h_{k+1}} (2m_{k+1} + m_{k+2}) = \frac{3}{h_k^2} (f_{k+1} - f_k) + \frac{3}{h_{k+1}^2} (f_{k+2} - f_{k+1}) \]
und nach Erweitern mit $h_k h_{k+1}$
\[ h_{k+1} m_k + 2( h_{k+1} + h_k ) m_{k+1} + h_k m_{k+2} = \frac{3h_{k+1}}{h_k} (f_{k+1} - f_k) + \frac{3 h_k}{h_{k+1}} ( f_{k+2} - f_{k+1} ). \]

Also gibt es $n-1$ Gleichungen für $n+1$ Größen $m_0, \ldots, m_n$:
\[ \begin{pmatrix} 
    \lambda_0 & 2         & \mu_0  &               &        & \\
              & \lambda_1 & 2      & \mu_1         &        & \\
              &           & \ddots & \ddots        & \ddots & \\
              &           &        & \lambda_{n-2} & 2      & \mu_{n-2}
   \end{pmatrix}
   \begin{pmatrix}
    m_0 \\ m_1 \\ \vdots \\ m_n
   \end{pmatrix}
   =
   \begin{pmatrix}
    r_0 \\ r_1 \\ \vdots \\ r_n
   \end{pmatrix}, \]
wobei
\[ \lambda_k := \frac{h_{k+1}}{h_k + h_{k+1}}, \quad \mu_k := \frac{h_k}{h_k + h_{k+1}}, \]
\[ r_k := \frac{3h_{k+1}}{h_k(h_k + h_{k+1})} (f_{k+1} - f_k) + \frac{3 h_k}{h_{k+1}(h_k + h_{k+1})} ( f_{k+2} - f_{k+1} ) \]
für $k = 0, \ldots, n-2$. Die Systemmatrix und die erweiterte Systemmatrix haben Rang $n-1$. Damit ist das System lösbar, besitzt aber keine eindeutige Lösung. Dafür können weitere Bedingungen gestellt werden:
\begin{enumerate}[a)]
 \item Natürliche Randbedingungen:
  \begin{equation}
   s''(x_0) = s''(x_n) = 0.
  \end{equation}
  Das ist gleichbedeutend mit
  \[ s''(x_0) = 6 a_0 (x_0 - x_0) + 2 b_0 = 0 \text{ und } s''_{n-1}(x_n) = 6 a_{n-1} (x_n - x_{n-1} ) + 2 b_{n-1} = 0.\]
  Also gilt
  \[ b_0 = 0 \text{ und } 3 a_{n-1} h_{n-1} + b_{n-1} = 0. \]
  Nutzt man noch die Darstellungen ($\circ$) und ($\circ\circ$) für $b_0$ sowie für $a_{n-1}, b_{n-1}$, so folgt
  \[ 2 m_0 + m_1 = \frac{3}{h_0}(f_1 - f_0) \text{ und } m_{n-1} + 2 m_n = \frac{3}{h_{n-1}} (f_n -f_{n-1}). \]
  Fügt man beide Gleichungen zu obigem System hinzu, erhält man ein lineares Gleichungssystem mit regulärer tridiagonaler Systemmatrix. Dieses kann in $O(n)$ Operationen gelöst werden.
 \item Vollständige Randbedingungen: Sind $f'(a)$ und $f'(b)$ bekannt, dann können die Bedingungen
  \begin{equation}
   s'(x_0) = f'(a), \quad s'(x_n) = f'(b)
  \end{equation}
  durch $m_0 := f'(a)$ und $m_n := f'(b)$ geeignet in das Gleichungssystem eingefügt werden, so dass man wieder eine tridiagonale reguläre Systemmatrix erhält.
 \item Periodische Spline-Interpolation: Falls
  \begin{equation}
   f'(a) = f'(b)
  \end{equation}
  und
  \[ f''(a) = f''(b) \]
  gilt, dann sind
  \begin{equation}
   s'(x_0) = s'(x_n) \text{ und } s''(x_0) = s''(x_n)
  \end{equation}
  sinnvolle Randbedingungen. Daraus lassen sich zwei zusätzliche Gleichungen zur Er\-gän\-zung des Gleichungssystems ableiten.
\end{enumerate}

\subsubsection{Minimaleigenschaft kubischer \texorpdfstring{$C^2$}{C2}-Interpolationssplines}
Durch
\[ \scalar{f}{g} := \int_a^b f(x) g(x) \diffop x \quad \text{bzw.} \quad \| g \|_2 := \sqrt{ \int_a^b [g(x)]^2 \diffop x } \]
für $f,g \in L^2[a,b]$ ist ein Skalarprodukt bzw. eine Norm in $L^2[a,b]$ definiert.

\begin{thm}
 Sei $f \in C^2[a,b]$, $\Delta$ eine Zerlegung von $[a,b]$ und $f(x_k) = g_k$, $k=0, \ldots, n$ für vorgegebene $g_k$. Sei $s \in S_3^2(\Delta)$ der Interpolationsspline zu den Werten $g_k$ mit den Randbedingungen a), b) oder c). Dann gilt
 \[ \| s'' \|^2_2 = \| f'' \|^2_2 - \| f'' - s'' \|^2_2 \le \| f'' \|^2_2. \]
\end{thm}
 Das heißt für $s \in S_3^2(\Delta)$ gilt
 \[ \| s'' \|^2_2 = \min_{f \in C^2[a,b]} \| f'' \|^2_2, \]
 für alle Interpolierenden $f$.

\begin{proof}
 Durch Einsetzen der Definition der Norm und Ausmultiplizieren (Integral ist linear!) erhält man
 \[ \int_a^b [f''(x)]^2 \diffop x - \int_a^b [ f''(x) - s''(x) ]^2 \diffop x = \int_a^b [s''(x) ]^2 \diffop x
    +2 \int_a^b [ f''(x) - s''(x) ] s''(x) \diffop x \]
Es wird nun $J: = \int_a^b [ f''(x) - s''(x) ] s''(x) \diffop x = 0$ gezeigt. Aufgrund der Struktur des Splines kann $J$ als Summe von Integralen über $\Delta$ dargestellt werden:
\[ J = \sum_{k=0}^{n-1} \int_{x_k}^{x_{k+1}} (f''(x) -s''_k(x) ) s''_k(x) \diffop x \]
 
Partielle Integration und Beachtung von $s'''_k = \mathrm{konst}$ auf $[x_k, x_{k+1}]$ liefert
\[ \int_{x_k}^{x_{k+1}} ( f''(x) - s''_k(x) ) s''_k(x) \diffop x = 
      \left. (f'(x) - s'_k(x) ) s''(x) \right|_{x_k}^{x_{k+1}}
    - \int_{x_k}^{x_{k+1}} (f'(x) -s'_k(x) ) s'''_k \diffop x \]
Für den zweiten Term ergibt sich
\begin{align*}
 \int_{x_k}^{x_{k+1}} (f'(x) -s'_k(x) ) s'''_k \diffop x 
 &= s_k''' \int_{x_k}^{x_{k+1}} (f'(x) -s'_k(x) ) \diffop x \\
 &= s_k''' (f(x_{k+1}) - s_k(x_{k+1}) - f(x_k) + s(x_k)) \\
 &= 0
\end{align*}
aufgrund der Interpolationsbedingungen (1.6). Damit folgt für $J$:
\[ J = ( f'(x) - s'_k(x) ) s''_k(x) |_a^b 
     = (f'(b) - s'(b) ) s''(b) - (f'(a) - s'(a))s''(a). \]
Aus den Randbedingungen (1.12), (1.13) bzw. (1.14) mit (1.15) kann jeweils $J=0$ gefolgert werden.
\end{proof}

\subsubsection{Interpolationsfehler bei der kubischen \texorpdfstring{$C^2$}{C2}-Interpolation}
\begin{thm}
 Sei $f \in C^2[a,b]$, $\Delta$ eine Zerlegung von $[a,b]$ und $f_k := f(x_k)$, $k = 0, \ldots, n$. Für einen Interpolationsspline $s \in S_3^2(\Delta)$, der die natürlichen, vollständigen oder periodischen Randbedingungen (bei letzteren gelte (1.14)) erfüllt, gilt
 \[ \| f - s \|_\infty \le \rez{2} h^{\frac{3}{2}} \| f'' \|_2 \]
 wobei $h := \max \{ h_0, \ldots, h_n \}$.
\end{thm}