\documentclass[
 a4paper,
 12pt,
 parskip=half
 ]{scrreprt}

\usepackage{../.tex/settings}

\usepackage{../.tex/mathpkgs}
\usepackage{../.tex/mathcmds}

\usepackage[numbers,with_chapter]{fancy_thm}

\swapnumbers
\theoremstyle{plain}
% \newtheorem{thm}{Satz}[section] % reset theorem numbering for each chapter
\newtheorem*{thm*}{Satz}
%\newtheorem{lem}[thm]{Lemma}    

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition} 
\newtheorem{folg}[thm]{Folgerung} 
\newtheorem{rmrk}[thm]{Bemerkung} 
\newtheorem{deno}[thm]{Bezeichnungen}
\newtheorem{exmp}[thm]{Beispiel}
%\newtheorem{aufg}[thm]{Aufgabe} 
\newtheorem{prgp}[thm]{} % Numbered paragraph

\newtheorem*{rmrk*}{Bemerkung}
\newtheorem*{exmp*}{Beispiel}
\newtheorem*{defn*}{Definition}
\newtheorem*{deno*}{Bezeichnung}
\newtheorem*{denos*}{Bezeichnungen}

\numberwithin{equation}{section}

\hypersetup{
  pdftitle={Stochastik},
  pdfauthor={Jonas Hippold},
  hidelinks
}

%opening
\title{%
  Vorlesung\\
  Stochastik Vertiefung\\
  Stationäre Prozesse
}
\subtitle{Sommersemester 2018}
\author{Vorlesung: Prof. Dr. Zoltán Sasvári\\Mitschrift: Jonas Hippold}

\begin{document}

\maketitle

\tableofcontents

\clearpage

%%\input{000_org}
\section*{Organisatorisches}
Aufgaben:\\
\url{http://www.math.tu-dresden.de/~sasvari/Download/StatProz/}

Prüfungsvorleistung: Vorrechnen von zwei Aufgaben in der Übung (evtl. auch
abgeben). 

Klausur für beide Teile des Moduls STOCHV: 110 Minuten

Teil Stationäre Prozesse: Formulierung von Definitionen, Sätzen; Beweise,
Aufgaben ähnlich zu den Übungsaufgaben.

Übung: Nico Uhlig

\subsection*{Literatur}
\begin{itemize}
\item H. Bauer: Wahrscheinlichkeitstheorie und Grundzüge der Maßtheorie
\item A. Rényi: Wahrscheinlichkeitsrechnung
\item K. D. Schmidt: Maß und Wahrscheinlichkeit
\end{itemize}

\chapter{Einleitung}
\section{Bezeichnungen}
\begin{prgp}
  Grundlegende Bezeichnungen:
  
  \begin{tabular}{lcl}
    $\nat$ & = & $\{1,2,3, \ldots\}$ \\
    $\nat_0$ & = & $\{0,1,2, \ldots\}$ \\
    $\integer$ & = & $\{ 0, \pm 1, \pm 2, \ldots \}$ \\
    $\rat$ & & rationale Zahlen \\
    $\real$ & & reelle Zahlen \\
    $\complex$ & & komplexe Zahlen \\
    $x_+$ & = & $\max(0,x)$, $x \in \real$ \\
    $\ind_A$ & & Indikatorfunktion der Menge $A$ \\
    $\delta_x$ & & Einpunkt- oder Dirac-Maß, das in $x$ konzentriert ist \\
    $\delta_{x,y}$ & & Kronecker'sche Delta-Funktion \\
    $\tilde{f}(x)$ & = & $\obar{f(-x)}$ \\
    $\pE(X)$ & & Erwartungswert der Zufallsgröße bzw. des Zufallsvektors $X$ \\
    $\intf_p(\real^d)$ & & Menge der komplexwertigen Lebesgue-messbaren
                           Funktionen $f$ auf $\real^d$, \\
           & & für die $|f|^p$ Lebesgue-integrierbar ist \\
    $\intf_p(\integer^d)$ & & Menge der komplexwertigen Funktionen $f$
                              auf $\integer^d$, \\
           & & die bezüglich des Zählmaßes integrierbar sind. \\
    $\| \cdot \|_p$ & & $L_p$-Norm \\
    $L_p(\real^d)$ & & $L_p$-Raum bezüglich des Lebesgue-Maßes auf $\real^d$ \\
    $L_p(\integer^d)$ & & $L_p$-Raum bezüglich des Zählmaßes auf $\integer^d$
  \end{tabular}
\end{prgp}

\begin{prgp}[Mehrdimensionale Bezeichnungen]
  Ist $x \in \real^d$ ($\complex^d$, $\nat_0^d$ usw.), so bezeichnet $x_i$ ($1
  \le i \le d$) die $i$-te Koordinate von $x$ und wir schreiben $x$ als $x =
  (x_1, \ldots, x_d)$. In Ausdrücken, wo Matrix-Operationen\footnote{%
    Zum Beispiel $Ax$ mit der Matrix $A$}
  vorkommen, betrachten wir $x$ als Spalten-Vektor. Das Nullelement von
  $\real^d$ wird auch mit 0 bezeichnet.

  Sei $x \in \real^d$ oder $\complex^d$ und $\alpha \in \nat_0^d$. Dann
  schreiben wir
  \begin{align*}
    x^\alpha
    &= x_1^{\alpha_1} \cdot x_1^{\alpha_2} \cdot \ldots \cdot x_d^{\alpha_d}, \\
    \| x \|
    &= \sqrt{|x_1|^2 + \ldots + |x_d|^2}, \\
    |\alpha|
    &= \alpha_1 + \ldots + \alpha_d, \\
    \alpha!
    &= \alpha_1! \cdot \ldots \cdot \alpha_d!, \\
    \binom{\alpha}{\beta}
    &= \frac{\alpha!}{(\alpha-\beta)! \beta!}
      = \binom{\alpha_1}{\beta_1} \cdot \ldots \cdot
      \binom{\alpha_d}{\beta_d}, \qquad \beta \le \alpha, \\
    D^\alpha
    &= \frac{\partial^{|\alpha|}}
      {\partial x_1^{\alpha_1} \cdot \ldots \cdot \partial x_d^{\alpha_d}}.
  \end{align*}
  Wenn $\alpha = (0, \ldots, 0)$, dann ist $x^\alpha := 1$.
\end{prgp}

\begin{rmrk*}
  Es gilt
  \[ D^\alpha D^\beta g = D^{\alpha + \beta} g, \]
  wobei $g$ eine beliebige reell- oder komplexwertige Funktion auf $\real^d$
  ist, für die die partiellen Ableitungen $D^{\alpha + \beta} g$ existieren.

  Es gilt $D^\alpha g = g$, wenn $\alpha = (0, \ldots, 0)$.

  Sei $\beta = (\beta_1, \ldots, \beta_d)$ ein anderes $d$-Tupel von
  nichtnegativen ganzen Zahlen. Wir schreiben $\beta \le \alpha$, wenn $\beta_j
  \le \alpha_j$ für alle $j$.

  Mit $\diffop x$, $\diffop \lambda(x)$ oder $\diffop \lambda_d(x)$ bezeichnen
  wir die Integration bezüglich des Lebesgue'schen Maßes $\lambda = \lambda_d$
  auf $\real^d$ und $(x,y)$ ist das Skalarprodukt von $x, y \in \real^d$ oder
  $\complex^d$.
\end{rmrk*}

\section{Historische Bemerkungen}
Geschichte:
\begin{itemize}
\item Albert Einstein (1914), stochastische Beschreibung von Molekülbewegungen
\item Älteste Zeitreihe: Zählung der Sonnenflecken, Maxima treten in regelmäßigen
\item Abständen auf, aber es gibt zufällige Abweichungen.
\item Khinchin (1934)
\end{itemize}
Zusammenfassung von L. Cohen: The History of Noise, IEEE Signal Proc. Mag.,
2005.

\chapter{Zufällige Felder zweiter Ordnung}
In diesem Abschnitt:
\begin{itemize}
\item $V \ne \emptyset$ ist eine beliebige nichtleere Menge.
\item $(\Omega, \mA, \pP)$ ist ein Wahrscheinlichkeitsraum.
\end{itemize}

\section{Zufallsfelder}
\begin{defn}
  Ein (reelles) \emph{Zufallsfeld} $Z$ auf $V$ ist eine Abbildung, die jedem
  Element $t \in V$ eine Zufallsgröße $Z(t)$ auf $\Omega$ zuordnet.

  Sind die Zufallsgrößen komplexwertig, dann heißt auch $Z$ komplexes
  Zufallsfeld.
\end{defn}

\begin{rmrk*}
  \begin{itemize}
  \item Anstelle von $Z(t)$ wird auch $Z_t$ geschrieben.
  \item Das Feld $Z$ wird auch mit $\{ Z_t \}$ oder $\{ Z_t : t \in V \}$ oder
    $\{ Z_t \}_{t \in V}$ bezeichnet.
  \item $V = \real, [0, \infty),[0,1]$: zeitstetiger stochastischer Prozess.
  \item $V = \nat, \nat_0$: zeitdiskreter stochastischer Prozess,
    \emph{Zeitreihe}.
  \item Die Funktionen $t \to Z_t(\omega)$ heißen \emph{Realisierungen} des
    Feldes.
  \end{itemize}
\end{rmrk*}

\begin{exmp}
  \begin{enumerate}[(a)]
  \item Sind $X_j$, $j \in \nat_0$, beliebige Zufallsgrößen (auf $\Omega$), so
    sind
    \[ \{X_j, j \in \nat_0\} \quad \text{und} \quad S_n = \sum_{j=0}^n X_j,
      \quad n \in \nat_0 \]
    Zeitreihen.
  \item Seien $a$ und $\varphi$ Zufallsgrößen und $u \in \real$. Dann ist
    \[ X_t = a \cdot \cos (ut + \varphi), \qquad t \in \real \]
    ein stochastischer Prozess auf $\real$, die \emph{harmonische Schwingung}
    mit zufälligen Parametern; Amplitude und Phase sind zufällig, die Frequenz
    ist gegeben.
  \end{enumerate}

  Allgemeiner:
  \[ X_t(\omega) = \sum_{j=1}^n a_j(\omega) \cdot
    \cos(u_j t + \varphi_j(\omega)), \qquad t \in \real. \]
  
  Viele Prozesse in Anwendungen lassen sich so modellieren. Die Menge $\{ u_1,
  \ldots, u_n \}$ heißt \emph{Spektrum} von $X$.

  Wichtige Aufgabe: Näherungsweise Bestimmung des Spektrums mit Hilfe von
  endlich vielen Beobachtungen $X_{t_1}(\omega), \ldots, X_{t_n}(\omega)$.

  Oft ist es vorteilhaft, komplexwertige Schwingungen zu betrachten:
  \[ Z_t = \sum_{j=1}^n c_j \cdot e^{i u_j t}, \qquad t \in \real. \]
  Hier ist $c_j$ eine komplexe Zufallsgröße. Wir schreiben $c_j$ in der Form
  \[ c_j = | c_j | \cdot e^{i \varphi_j}, \]
  wobei $\varphi_j$ eine reelle Zufallsgröße ist. Dann gilt
  \[ Z_t = \sum_{j=1}^n |c_j| \cdot e^{i(u_j t + \varphi_j)} \]
  und 
  \begin{align*}
    \Re Z_t &= \sum_{j=1}^n |c_j| \cdot \cos (u_j t + \varphi_j) \\
    \Im Z_t &= \sum_{j=1}^n |c_j| \cdot \sin (u_j t + \varphi_j)
  \end{align*}
\end{exmp}

\begin{defn}
  Ein komplexes \emph{Zufallsfeld zweiter Ordnung} $Z$ auf $V$ ist eine
  Abbildung $Z: V \to \intf_2(\Omega,\mA,\pP)$.

  Wird $\intf_2$ durch $\intf_2^r$ ersetzt, so heißt $Z$ ein \emph{reelles
    Zufallsfeld zweiter Ordnung}.
\end{defn}

In beiden Fällen bezeichnet $(\cdot, \cdot)$ das Skalarprodukt
\[ (X,Y) = \int_{\Omega} X \cdot \obar{Y} \diffop \pP = \pE(X \cdot \obar{Y}). \]
Speziell: $\pE(X) = (X,\ind)$

Weiterhin gilt:
\[ (X,Y) = (\ind, \obar{X} \cdot Y) = (X \cdot \obar{Y}, \ind). \]
$\| \cdot \|$ bezeichnet die Norm
\[ \| X \| := \sqrt{(X,X)}. \]
Damit können wir eine Metrik definieren:
\[ d(X,Y) := \| X - Y \|. \]
Cauchy-Schwartz:
\[ |\pE(X \cdot Y)| \le \| X \| \cdot \| Y \|. \]
Orthogonalität:
\[ X \perp Y \quad :\Leftrightarrow \quad (X,Y) = \pE(X \cdot \obar{Y}) = 0.\]
Zwei Zufallsgrößen mit Erwartungswert 0 sind genau dann unkorreliert, wenn sie
orthogonal sind.

Wir werden ein Zufallsfeld $Z$ zweiter Ordnung auch als eine Abbildung $Z : V
\to L_2(\Omega, \mA, \pP)$ oder $Z : V \to L^r_2(\Omega, \mA, \pP)$ betrachten.
In diesem Fall ist $Z_t$ keine Zufallsgröße, sondern eine Äquivalenzklasse von
Zufallsgrößen.

Vorteil dieser Betrachtung: $L_2$ und $L_2^r$ sind Hilberträume.

Vorsicht: Zum Beispiel ist $Z_t(\omega)$ kein sinnvoller Ausdruck, aber
\[ (Z_t, Z_s) = \int_\Omega Z_t \cdot \obar{Z}_s \diffop \pP, \]
da $\int X \diffop \pP = \int Y \diffop \pP$, wenn $X = Y$ fast sicher.

Ausnahme: $Z_t(\omega)$ kann doch sinnvoll sein.
\[ \pP(A) = 0  \qRq A = \emptyset \tag{$*$} \]
gilt genau dann, wenn $\Omega = \{ \omega_1, \omega_2, \ldots \}$ höchstens
abzählbar viele Elemente hat und $\pP(\omega_j) > 0$ für alle $j$. Das heißt,
aus ($*$) folgt $\pP(\omega) > 0$ für alle $\omega \in \Omega$ und damit auch,
dass $\Omega$ höchstens abzählbar unendlich ist, weil
\[ \Omega = \bigcup_n
  \underbrace{%
    \left\{ \omega : \pP(\{\omega\}) \ge \rez{n} \right\}
  }_{%
    \text{Höchstens $n$ Elemente}
  }.
\]

\begin{deno*}
  Sei $Z$ ein Feld auf $V$ mit Werten in $L_2(\Omega,\mA,\pP)$ oder in
  $\intf(\Omega,\mA,\pP)$. Mit $H(Z)$ bzw. $\mathcal{H}(Z)$ bezeichnen wir die
  abgeschlossene lineare Hülle (bezüglich $\| \cdot \|$) der ``Vektoren'' $Z_t$,
  $t \in V$:
  \[ H(Z) = \left\{
      \sum_{j=1}^n c_j Z_{t_j} : c_j \in \complex, t_j \in V, n \in \nat
    \right\}^-. \]
  Sie besteht also aus allen Zufallsgrößen, die man durch endliche
  Linearkombinationen erzeugen kann. $\mathcal{H}(Z)$ analog.

  Im reellen Fall schreibt man $H^r(Z)$, $\mathcal{H}^r(Z)$.

  $H(Z)$ und $H_2^r(Z)$ sind Hilberträume.
\end{deno*}

\begin{exmp}
  \begin{enumerate}[(a)]
  \item Sei $X \in L_2(\pP)$ und $f: V \to \complex$ beliebig. Dann ist
    \[ Z(t) = f(t) \cdot X, \quad t \in V \]
    ein Feld zweiter Ordnung. Weiterhin gilt:
    \[ H(Z) = \complex \cdot X. \]
    Falls $f \equiv 0$, dann ist $H(Z) = \{0\}$.

    \emph{Allgemeiner:} Seien $X_n \in L_2(\pP)$, $n \in \nat$, unkorreliert mit
    $\pE(X_n) = 0$ und Varianz 1, das heißt $\| X_n \| = 1$. Dann ist $\{X_1,
    X_2, \ldots \}$ ein \emph{orthonormiertes System}\footnote{%
      Das heißt $(X_i, X_j) = 0$ für $i \ne j$ und $(X_j, X_j) = 1$.
    } im Hilbertraum $L_2(\pP)$.

    Weiterhin seien $f_n : V \to \complex$ so, dass
    \[ \sum_{n=1}^\infty |f_n(t)|^2 < \infty. \]
    Dann ist
    \[ Z(t) = \sum_{n=1}^\infty f_n(t) \cdot X_n \]
    ein Feld zweiter Ordnung.
  \item Umkehrung von (a). Spezialfall: $L_2(\pP)$ separabel\footnote{%
      Ein metrischer Raum $M$ heißt \emph{separabel}, wenn eine abzählbare
      Teilmenge $N = \{m_1, m_2, \ldots \} \subset M$ existiert mit $\obar{N} =
      M$. Zum Beispiel ist $\real^d$ separabel, $N = \rat^d$.
    }. Dann existiert eine abzählbare orthonormale Basis
    $\{ e_1, e_2, \ldots \}$. Dann gilt:
    \[ Z(t) = \sum_{n=1}^\infty f_n(t) \cdot e_n, \quad t \in V, \]
    wobei $f_n(t) = (Z_t, e_n)$. Weiterhin ist
    \[ \| Z(t) \|^2 = \sum_{n=1}^\infty | f_n(t) |^2. \]
  \end{enumerate}
\end{exmp}

Bemerkung:
Sei $e_1, e_2, \ldots \in \mathcal{H}$ Hilbert-Raum ein orthonormiertes
System.
\[ \sum_{j=1}^\infty c_j e_j := \lim_{n \to \infty} \sum_{j=1}^n c_j e_j, \]
falls der Grenzwert existiert.

$\lim_n x_n$ existiert $\Leftrightarrow$ $\{ x_n\}$ ist eine Cauchy-Folge in
beliebigen metrischen Räumen.
\[ x_n = \sum_{j=1}^n c_j \cdot e_j \]
ist eine Cauchy-Folge, wenn
\[ \| x_n - x_m \| \xrightarrow{n,m \to \infty} 0. \]
Sei zum Beispiel $m \ge n$, dann ist
\[ x_n - x_m = \sum_{j=n}^m c_j e_j. \]
Also folgt
\[ \| x_n - x_m \|^2 = \sum_{j=n}^m |c_j|^2 \xrightarrow{m,n \to \infty}
  0 \]
$\Leftrightarrow$ $\sum_{j=1}^\infty |c_j|^2 < \infty$.

Wiederholung:
Feld 2. Ordnung ist eine Abb. $V \to \intf^2(\pP)$ bzw. $L^2(\pP)$.

Das Skalarprodukt $(X,Y) := \pE( X \cdot \obar{Y})$ ist für beide Varianten
sinnvoll. Es ist positiv semidefinit:
\[ (X,X) \ge 0, \]
für $L^2(\pP)$ sogar
positiv definit:
\[ (X,X) > 0 \qquad \text{für} \qquad X \ne 0. \]

Bekannt aus der Maßtheorie: Wenn $X \ge 0$ und $\int X \diffop \pP = 0$, dann
ist $X = 0$ fast sicher.

\begin{defn}
  Seien $X$ und $Y$ Felder 2. Ordnung auf $V$ mit dem selben $\pW$-Raum
  $(\Omega, \mA, \pP)$. Gilt $(X(t),Y(s)) = 0$ für alle $t,s \in V$, so heißen
  $X$ und $Y$ orthogonal: $X \perp Y$.

  Zum Beispiel $X(t)$, $Y(s)$ unabhängig und $\pE(X(t)) = \pE(Y(s)) = 0$, dann
  \[ (X(t),Y(s)) = \pE(X(t) \obar{Y(s)}) = \pE(X(t)) \pE(\obar{Y(s)}) = 0. \]
\end{defn}

\begin{defn}
  Sei $Z$ ein Zufallsfeld auf $V$ und $t_1, \ldots, t_n \in V$. Dann ist $(Z_{t_1},
  \ldots, Z_{t_n})$ ein Zufallsvektor und besitzt eine Verteilung. Verteilungen
  dieser Form heißen \emph{endlichdimensionale Verteilungen} von $Z$.

  Ist $Z$ reell oder komplex und sind alle endlichdimensionalen Verteilungen
  Gauß'sch, so nennt man auch $Z$ \emph{Gauß'sch}.

  Simples Beispiel: $V = \{1\}$, $Z_1$ beliebige normalverteilte Zufallsgröße.
  Oder $V = \{1, 2, \ldots\} = \nat$, $Z_1, Z_2, \ldots$ Gauß'sch und
  unabhängig. Dann ist die Verteilung von $(Z_1, \ldots, Z_n)$ das Produkt der
  Verteilungen der $Z_j$.
\end{defn}

\section{Die Korrelationsfunktion}
\begin{defn}
  Sei $Z$ ein Feld zweiter Ordnung auf $V$. Die Funktion $C: V \times V \to
  \complex$, die durch
  \[ C(X,Y) = \pE[ Z(X) \cdot \obar{Z(Y)} ) = ( Z(X), Z(Y) ] \]
  definiert ist, heißt \emph{Korrelationsfunktion} von $Z$.

  Die \emph{Kovarianzfunktion} ist durch
  \[ \sigma(X,Y) = \pE [ (Z(X) - M(X)) \cdot \obar{(Z(Y) - M(Y))}] \]
  definiert, wobei $M(X) := \pE(Z(X))$ der Mittelwert bzw. Erwartungswert des
  Feldes ist.
\end{defn}

Es ist leicht zu sehen:
\[ \sigma( X, Y ) = C(X,Y) - M(X) \cdot \obar{M(Y)}. \]
Wenn $M \equiv 0$, dann ist $\sigma = C$.

\begin{thm}
  Eine komplexwertige (reellwertige) Funktion $K$ auf $V \times V$ ist genau
  dann die Kovarianz- oder Korrelationsfunktion eines komplexen (reellen) Felder
  zweiter Ordnung, wenn für beliebige $X_1, \ldots, X_n \in V$, $n \in \nat$,
  die Matrix
  \[ (K(X_i,X_j))_{i,j=1}^n \]
  positiv semidefinit (positiv semidefinit und symmetrisch) ist.
\end{thm}

\begin{proof}
  Nehmen wir an, dass $K$ die Kovarianzfunktion des Feldes $Z$ ist. Dann gilt
  \[ \sum_{i,j=1}^n K(X_i, X_j) a_i \obar{a_j}
    = \sum_{i,j=1}^n \pE[ (Z(X_i) - M(X_i)) \cdot \obar{(Z(X_j) - M(X_j))}] a_i
    \obar{a_j}. \]
  Es gilt
  \[ \pE \left| \sum_{j=1}^n (Z(X_j) - M(X_j)) a_j \right|^2 \ge 0, \]
  denn
  \[ |w|^2 = w \cdot \obar{w} \]
  für alle $w \in \complex$, also auch für
  \[ w := \sum_{j=1}^n (Z(X_j) - M(X_j)) a_j. \]

  Die Fälle $Z$ reell und $K$ Korrelationsfunktion folgen analog. Der zweite
  Teil des Beweises folgt aus Satz 2.2.4.
\end{proof}

\begin{lem} %% 2.2.3
  Sei $C = (c_{jk})_{j,k=1}^n$ eine positiv semidefinite Matrix und
  \[ A = (a_{jk}) = \Re C, \qquad B = (B_{jk}) = \Im C. \]
  Dann ist die $(2n) \times (2n)$-Matrix
  \[ D = (d_{jk}) = \begin{bmatrix}
      A & -B \\ B & A
    \end{bmatrix}
  \]
  ebenfalls positiv semidefinit.
\end{lem}

\begin{proof}
  Aus $c_{jk} = \obar{c_{kj}}$ folgt, dass $A^\top = A$ ud $B^\top = -B$, damit
  muss $D$ symmetrisch sein.

  Für beliebige $r_1, \ldots, r_{2n} \in \real$ gilt
  \[ 0 \le \sum_{j,k = 1}^n c_{jk}( r_j - i r_{n+j}) \cdot (r_k + r_{n+k}), \]
  da $C$ positiv semidefinit ist.

  Es gilt
  \begin{align*}
    &\sum_{j,k=1}^n c_{jk} (r_j r_k + r_{n+j} r_{n+k}) - i \sum_{j,k=1}^n c_{jk}
    (r_{n+j} r_k + r_j r_{n+k}) \\
    = &\sum_{j,k=1}^n a_{jk} (r_j r_k + r_{n+j} r_{n+k}) + \sum_{j,k=1}^n b_{jk}
        (r_{n+j} r_k + r_j r_{n+k}) \\
    = &\sum_{j,k = 1}^n d_{jk} r_j r_k.
  \end{align*}
  Die erste Summe ist reell, die zweite rein imaginär, da
  $c_{jk} = \obar{c_{kj}}$. Die zweite Identität folgt aus der Definition von
  $D$.
\end{proof}

\clearpage

\begin{thm}
  Sei $M : V \to \complex$ beliebig und $K : V \times V \to \complex$ so, dass
  für beliebige $x_1, \ldots, x_n \in V$ die Matrix
  \[ (K(x_i, x_j) )_{i,j=1}^n \tag{1} \]
  positiv semidefinit ist. Dann existiert ein Gauß-Feld $Z$ auf $V$ mit
  \begin{align*}
    \pE(Z(x))
    &= M(x), \tag{2} \\
    \pE(Z(x) \cdot \obar{Z(y)}) - M(x) \obar(M(y))
    &= K(x,y), \tag{3} \\
    \pE(Z(x) \cdot Z(y))
    &= M(x) \cdot M(y). \tag{4}
  \end{align*}
  Sind $M$ und $K$ reellwertig, so existiert ein reelles Gauss-Feld $Z$ so, dass
  (2) und (3) gelten.
\end{thm}

\begin{rmrk*}
  Wenn ein Feld $Z$ (2) und (3) erfüllt, dann ist
  \[ \pE[ (Z(x) - M(x))^2 ] = 0. \]
  Wenn also $Z$ reell ist, folgt $Z(x) = M(x)$.
\end{rmrk*}

\appendix

\input{oo_ortho}

\input{bb_semidef}

\input{gg_char}

\end{document}